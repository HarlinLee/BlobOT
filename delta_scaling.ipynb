{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarlinLee/BlobOT/blob/main/delta_scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cVZVi6u6KvL",
        "outputId": "f8bb614b-9707-42b8-b1ec-30f8611c6cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "230526\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/BlobOT-final\"\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, DRIVE_PATH)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "import datetime\n",
        "date = datetime.datetime.today().strftime(\"%y%m%d\")\n",
        "print(date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbDXLfZkYnJD",
        "outputId": "e2dd6efb-af18-4bed-a9c4-f11849c1b601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        " dev = \"cuda:0\" \n",
        "else: \n",
        " dev = \"cpu\" \n",
        "device = torch.device(dev) \n",
        "print(device)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkN_Pm5TYoe4"
      },
      "outputs": [],
      "source": [
        "from BlobOT import KE, NLE, NLE_pos, Model\n",
        "from BlobOT import draw_straight_lines, EarlyStopping, endpoint_cost\n",
        "\n",
        "def blobLoss(X, model, positive=False):\n",
        "  if not positive:\n",
        "    return KE(X, model.z, model.params) + NLE(X, model.w, model.params)\n",
        "  else:\n",
        "    return KE(X, model.z, model.params) + NLE_pos(X, model.w, model.params)\n",
        "\n",
        "\n",
        "def training_loop(model, optimizer, n=10000):\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True,\n",
        "                                                         patience=2, threshold=0,\n",
        "                                                         factor=0.2)\n",
        "  early_stopping = EarlyStopping(patience=5)\n",
        "\n",
        "  losses=[]\n",
        "\n",
        "  for i in range(1, 1+n):\n",
        "    preds = model()\n",
        "    loss = blobLoss(preds, model)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    scheduler.step(loss)\n",
        "    early_stopping(loss)\n",
        "\n",
        "    losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early stopping at\", i, 'out of', n)\n",
        "      return np.array(losses)\n",
        "\n",
        "  return np.array(losses)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVsHQA7jSJ4l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import pdist, cdist, squareform\n",
        "\n",
        "def plot_1d_paths(X, w, z, params=None, figsize=(10, 5), **kwargs):\n",
        "  fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
        "  if X.shape[-1] <= params['L']:\n",
        "    X = np.concatenate((np.expand_dims(z, axis=-1), X), axis=-1)\n",
        "  \n",
        "  t = params['dt']*np.arange(params['L']+1)\n",
        "\n",
        "  for p in range(X.shape[0]):\n",
        "    traj = X[p].T\n",
        "    ax[0].plot(traj[:, 0], t,  color=[.5, .5, 1])\n",
        "  \n",
        "  y = X[:, :, -1]\n",
        "  ax[0].scatter(X[:, 0, 0], np.zeros_like(X[:, 0, 0]), marker='+', c='b', label='Source')\n",
        "  ax[0].scatter(w[:, 0], 1 + np.zeros_like(w[:, 0]), marker='x', c='r', label='Target')\n",
        "  ax[0].scatter(y[:, 0], 1 + np.zeros_like(y[:, 0]), marker='.', c='k', label='x(1)')\n",
        "  ax[0].set(xlabel='Coordinate 1', ylabel='Time (t)', **kwargs)\n",
        "  \n",
        "  ax[1].scatter(w[:, 0], 1 + np.zeros_like(w[:, 0]), marker='x', c='r', label='Target')\n",
        "  ax[1].scatter(y[:, 0], 1 + np.zeros_like(y[:, 0]), marker='.', c='k', label='x(1)')\n",
        "  \n",
        "  for p in range(X.shape[0]):\n",
        "    traj = X[p].T\n",
        "    ax[1].plot(traj[(params['L']-1):, 0], t[(params['L']-1):],  color=[.5, .5, 1])\n",
        "\n",
        "  ax[1].set(xlabel='Coordinate 1', ylabel='Time (t)',**kwargs)\n",
        "  plt.legend()\n",
        "  plt.tight_layout(pad=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Oa7pZwDfce"
      },
      "source": [
        "# as N $\\to \\infty$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "k53PurLmSJ7d",
        "outputId": "03141d6f-8d35-42cd-f2ba-98069f165ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "{'d': 1, 'dt': 0.25, 'L': 4, 'lr': 2.9999999999999997e-05, 'n': 2000000, 'eps': 0.01, 'N': 5, 'delta': 0.20324491825346513}\n",
            "Epoch 324360: reducing learning rate of group 0 to 6.0000e-06.\n",
            "Epoch 324366: reducing learning rate of group 0 to 1.2000e-06.\n",
            "Epoch 324372: reducing learning rate of group 0 to 2.4000e-07.\n",
            "Epoch 324377: reducing learning rate of group 0 to 4.8000e-08.\n",
            "Early stopping at 324379 out of 2000000\n",
            "endpoint cost: 0.005568993608181547\n",
            "\n",
            "{'d': 1, 'dt': 0.25, 'L': 4, 'lr': 2.9999999999999997e-05, 'n': 2000000, 'eps': 0.01, 'N': 10, 'delta': 0.10232929922807542}\n",
            "Epoch 602173: reducing learning rate of group 0 to 6.0000e-06.\n",
            "Epoch 602177: reducing learning rate of group 0 to 1.2000e-06.\n",
            "Early stopping at 602179 out of 2000000\n",
            "endpoint cost: 0.0026190502398721286\n",
            "\n",
            "{'d': 1, 'dt': 0.25, 'L': 4, 'lr': 2.9999999999999997e-05, 'n': 2000000, 'eps': 0.01, 'N': 20, 'delta': 0.05152052789556263}\n",
            "Epoch 1129157: reducing learning rate of group 0 to 6.0000e-06.\n",
            "Epoch 1129161: reducing learning rate of group 0 to 1.2000e-06.\n",
            "Epoch 1129165: reducing learning rate of group 0 to 2.4000e-07.\n",
            "Early stopping at 1129167 out of 2000000\n",
            "endpoint cost: 0.001959874172315229\n",
            "\n",
            "{'d': 1, 'dt': 0.25, 'L': 4, 'lr': 2.9999999999999997e-05, 'n': 2000000, 'eps': 0.01, 'N': 40, 'delta': 0.025939440753144395}\n",
            "Epoch 1363325: reducing learning rate of group 0 to 6.0000e-06.\n",
            "Epoch 1363333: reducing learning rate of group 0 to 1.2000e-06.\n",
            "Epoch 1363341: reducing learning rate of group 0 to 2.4000e-07.\n",
            "Early stopping at 1363343 out of 2000000\n",
            "endpoint cost: 0.0010932053064116696\n",
            "\n",
            "{'d': 1, 'dt': 0.25, 'L': 4, 'lr': 2.9999999999999997e-05, 'n': 2000000, 'eps': 0.01, 'N': 80, 'delta': 0.013059931915873091}\n",
            "endpoint cost: 0.0006452875567368809\n",
            "\n",
            "{'d': 1, 'dt': 0.25, 'L': 4, 'lr': 2.9999999999999997e-05, 'n': 2000000, 'eps': 0.01, 'N': 100, 'delta': 0.010471285480508996}\n",
            "endpoint cost: 0.0005438067318404318\n"
          ]
        }
      ],
      "source": [
        "rng = np.random.RandomState(23)\n",
        "\n",
        "d = 1\n",
        "dt = 0.25\n",
        "L = int(1/dt)\n",
        "\n",
        "eps = 1e-2\n",
        "lr = 3*eps/1e3\n",
        "gd_steps = 2*int(1e6) # run it longer for straighter lines\n",
        "\n",
        "params = {'d': d, 'dt': dt, 'L': L, 'lr': lr, 'n': gd_steps, 'eps': eps}\n",
        "\n",
        "endpoint_costs = []\n",
        "\n",
        "def T(x):\n",
        "  return 0.5*x+2\n",
        "\n",
        "Ns = [5, 10, 20, 40, 80, 100]\n",
        "\n",
        "for N in Ns:\n",
        "  print()\n",
        "\n",
        "  params['N'] = N\n",
        "  params['delta'] = N**(-0.99)\n",
        "  \n",
        "  print(params)\n",
        "\n",
        "  # sample from source and target distributions\n",
        "  z = np.linspace(0, 1, N, endpoint=True).reshape((N, d)) # can be changed to random samples\n",
        "  w = T(z)\n",
        "\n",
        "  w_tensor = torch.tensor(w)\n",
        "  z_tensor = torch.tensor(z)\n",
        "\n",
        "  # initialize to center of target distribution\n",
        "  y0 = np.ones((N, d))*(T(0) + T(1))/2\n",
        "  X0 = draw_straight_lines(z, y0, params['L']+1)\n",
        "\n",
        "  # run gradient descent\n",
        "  m = Model(torch.from_numpy(X0), w_tensor, z_tensor, params)\n",
        "  opt = torch.optim.SGD(m.parameters(), lr=params['lr'])\n",
        "  losses = training_loop(m, opt, n=params['n'])\n",
        "  \n",
        "  # calculate error from OT map\n",
        "  X = m.X.data.numpy()\n",
        "  y = X[:,:,-1]\n",
        "\n",
        "  c1 = endpoint_cost(y, T(z))\n",
        "  endpoint_costs.append(c1)\n",
        "\n",
        "  print('endpoint cost:', c1)\n",
        "\n",
        "  plot_1d_paths(X, w, z, params)\n",
        "  plt.savefig(os.path.join(DRIVE_PATH, f'delta_scaling_{date}-{N}.pdf'), dpi=100)\n",
        "  plt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NGKxcAQNLmq6"
      },
      "outputs": [],
      "source": [
        "np.savez(os.path.join(DRIVE_PATH, f'delta_scaling_{date}.npz'), \n",
        "         endpoint_costs=np.array(endpoint_costs), \n",
        "         Ns=Ns, \n",
        "         params=params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyC1Lkd5b2GX"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NG4Ite_gQPF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/BlobOT-final\"\n",
        "\n",
        "import datetime\n",
        "date = datetime.datetime.today().strftime(\"%y%m%d\")\n",
        "print(date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgYiBLdtffaH"
      },
      "outputs": [],
      "source": [
        "data = np.load(os.path.join(DRIVE_PATH, f'delta_scaling_230525.npz'), allow_pickle=True)\n",
        "print(data.files)\n",
        "endpoint_costs = data['endpoint_costs']\n",
        "Ns = data['Ns']\n",
        "params = data['params']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqyTCk0goEaL"
      },
      "outputs": [],
      "source": [
        "p = np.polynomial.polynomial.polyfit(np.log10(Ns), np.log10(endpoint_costs), 1)\n",
        "print(p)\n",
        "\n",
        "plt.figure(1, figsize=(8, 6))\n",
        "plt.plot(Ns, endpoint_costs, linewidth=2, marker='o', label='Mean error')\n",
        "plt.plot(Ns, 10**(p[0])*(Ns**p[1]), linestyle='--', color='r', label=f'Linear fit (slope = {np.around(p[1], 2)}, intercept={np.around(p[0], 2)})')\n",
        "plt.xlabel(r'Number of particles ($N$)', fontsize=16)\n",
        "plt.ylabel(r'Error from OT map at $t=1$', fontsize=16)\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xticks(Ns, [5, 10, 20, 40, 80, 100], fontsize=14)\n",
        "plt.yticks([5*1e-3, 3*1e-3, 1e-3], [5*1e-3, 3*1e-3, 1e-3], fontsize=14)\n",
        "plt.legend(fontsize=14)\n",
        "plt.title(r'$\\delta= N^{-0.99}$', fontsize=16)\n",
        "plt.tight_layout(pad=0)\n",
        "plt.savefig(os.path.join(DRIVE_PATH, f'delta_scaling_{date}_loglog_endpoint.pdf'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WirlFto2QsdX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvycfuHiainbVROHTDzTig",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}